/// <reference lib="dom" />
'use client';

import { useState, useEffect, useRef } from 'react';
import Tesseract from 'tesseract.js';

declare global {
  interface Window {
    SpeechRecognition: any;
    webkitSpeechRecognition: any;
  }

  interface SpeechRecognition extends EventTarget {
    lang: string;
    interimResults: boolean;
    continuous: boolean;
    onresult: ((event: SpeechRecognitionEvent) => void) | null;
    onerror: ((event: Event) => void) | null;
    onend: (() => void) | null;
    start(): void;
    stop(): void;
  }

  interface SpeechRecognitionEvent extends Event {
    results: SpeechRecognitionResultList;
    resultIndex: number;
  }
}

const AIDashboard = () => {
  const [userQuery, setUserQuery] = useState('');
  const [imageFile, setImageFile] = useState<File | null>(null);
  const [aiThinking, setAiThinking] = useState(false);
  const [isListening, setIsListening] = useState(false);
  const [isSpeaking, setIsSpeaking] = useState(false);
  const [messages, setMessages] = useState<{ role: 'user' | 'ai'; content: string }[]>([]);

  const recognitionRef = useRef<SpeechRecognition | null>(null);
  const messagesEndRef = useRef<HTMLDivElement>(null);

  useEffect(() => {
    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });
  }, [messages, aiThinking]);

  useEffect(() => {
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    if (!SpeechRecognition) return;

    const recognition = new SpeechRecognition();
    recognition.lang = 'en-IN';
    recognition.interimResults = true;
    recognition.continuous = false;

    recognition.onresult = (event: SpeechRecognitionEvent) => {
      let fullTranscript = '';
      for (let i = event.resultIndex; i < event.results.length; i++) {
        fullTranscript += event.results[i][0].transcript;
      }
      setUserQuery(fullTranscript);
    };

    recognition.onerror = () => setIsListening(false);
    recognition.onend = () => setIsListening(false);

    recognitionRef.current = recognition;
  }, []);

  const speakText = (text: string) => {
    const synth = window.speechSynthesis;
    synth.cancel();

    const utterance = new SpeechSynthesisUtterance(text);
    utterance.lang = 'en-IN';
    utterance.pitch = 1.1;
    utterance.rate = 0.93;
    utterance.volume = 1;

    const voices = synth.getVoices();
    const indianVoice = voices.find(v => v.lang === 'en-IN') || voices.find(v => v.name.includes('Google'));
    if (indianVoice) utterance.voice = indianVoice;

    utterance.onstart = () => setIsSpeaking(true);
    utterance.onend = () => setIsSpeaking(false);

    if (voices.length === 0) {
      synth.onvoiceschanged = () => synth.speak(utterance);
    } else {
      synth.speak(utterance);
    }
  };

  const stopVoice = () => {
    window.speechSynthesis.cancel();
    setIsSpeaking(false);
  };

  const pauseVoice = () => {
    if (window.speechSynthesis.speaking) {
      window.speechSynthesis.pause();
    }
  };

  const resumeVoice = () => {
    if (window.speechSynthesis.paused) {
      window.speechSynthesis.resume();
    }
  };

  const toggleListening = () => {
    if (!recognitionRef.current) return;
    if (isListening) {
      recognitionRef.current.stop();
    } else {
      recognitionRef.current.start();
    }
    setIsListening((prev) => !prev);
  };

  const handleSubmit = async (e: React.FormEvent) => {
    e.preventDefault();
    if (!userQuery && !imageFile) return;

    let finalQuery = userQuery;

    if (imageFile) {
      const result = await Tesseract.recognize(imageFile, 'eng');
      finalQuery = result.data.text.trim();
    }

    setMessages(prev => [...prev, { role: 'user', content: finalQuery }]);
    setUserQuery('');
    setAiThinking(true);

    const res = await fetch('/api/openrouter', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ message: finalQuery }),
    });

    const data = await res.json();
    const cleaned = data.response
      .replace(/As an AI language model,?/gi, '')
      .replace(/I cannot/gi, "Here's what we can understand:")
      .replace(/I'm just an AI/gi, "Here's my explanation:")
      .replace(/Based on my training/gi, "According to my knowledge");

    setMessages(prev => [...prev, { role: 'ai', content: cleaned }]);
    setAiThinking(false);
  };

  return (
    <div className="flex flex-col h-screen bg-white">
      <header className="text-center py-4 shadow-md border-b bg-gradient-to-r from-blue-600 to-purple-600 text-white">
        <h1 className="text-xl md:text-2xl font-bold">
          ğŸ“Š ValuCommerce AI â€” Your Smart Commerce Expert
        </h1>
      </header>

      <main className="flex-grow overflow-y-auto p-4 md:p-6 bg-gray-50">
        <div className="max-w-3xl mx-auto space-y-4">
          {messages.length === 0 && (
            <p className="text-center text-gray-400">
              Ask anything related to GST, Taxation, Business Law, CA, Economicsâ€¦
            </p>
          )}

          {messages.map((msg, i) => (
            <div
              key={i}
              className={`relative max-w-[80%] px-4 py-3 rounded-lg shadow-sm whitespace-pre-wrap ${
                msg.role === 'user'
                  ? 'bg-blue-100 text-blue-900 ml-auto text-right'
                  : 'bg-gray-100 text-gray-800 mr-auto text-left'
              }`}
            >
              {msg.content}
              {msg.role === 'ai' && (
                <div className="absolute bottom-2 right-2 flex gap-2">
                  <button onClick={() => speakText(msg.content)} title="Play">â–¶ï¸</button>
                  <button onClick={pauseVoice} title="Pause">â¸ï¸</button>
                  <button onClick={resumeVoice} title="Resume">ğŸ”</button>
                  <button onClick={stopVoice} title="Stop">â¹ï¸</button>
                </div>
              )}
            </div>
          ))}

          {aiThinking && (
            <div className="flex items-center gap-2 text-gray-500 text-sm">
              <div className="w-4 h-4 border-2 border-blue-500 border-t-transparent rounded-full animate-spin"></div>
              <span>ValuCommerce AI is thinking...</span>
            </div>
          )}

          <div ref={messagesEndRef} />
        </div>
      </main>

      <form onSubmit={handleSubmit} className="w-full border-t bg-white p-4 flex items-center gap-2">
        <label className="cursor-pointer text-2xl">
          ğŸ“
          <input type="file" accept="image/*" className="hidden" onChange={(e) => setImageFile(e.target.files?.[0] || null)} />
        </label>

        <input
          value={userQuery}
          onChange={(e) => setUserQuery(e.target.value)}
          placeholder="Speak or type your commerce doubt..."
          className="flex-grow px-4 py-2 border border-gray-300 rounded-lg focus:outline-none focus:ring-2 focus:ring-blue-400 text-gray-900"
        />

        <button type="button" onClick={toggleListening} className={`text-xl px-3 py-2 rounded-lg transition ${isListening ? 'bg-red-600 text-white animate-pulse' : 'bg-gray-200 hover:bg-gray-300 text-gray-700'}`}>
          ğŸ¤
        </button>

        <button type="submit" className="bg-blue-600 hover:bg-blue-700 text-white px-4 py-2 rounded-lg font-medium">
          Ask â¤
        </button>
      </form>
    </div>
  );
};

export default AIDashboard;
